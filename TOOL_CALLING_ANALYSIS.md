# Google Gemini CLI å·¥å…·è°ƒç”¨æ¥å£æ”¯æŒåˆ†æ

## ğŸ“‹ ç›®å½•
1. [ç°çŠ¶åˆ†æ](#ç°çŠ¶åˆ†æ)
2. [OpenAI å·¥å…·è°ƒç”¨æ ¼å¼](#openai-å·¥å…·è°ƒç”¨æ ¼å¼)
3. [Gemini å·¥å…·è°ƒç”¨æ ¼å¼](#gemini-å·¥å…·è°ƒç”¨æ ¼å¼)
4. [æ ¼å¼å¯¹æ¯”](#æ ¼å¼å¯¹æ¯”)
5. [å®ç°æ–¹æ¡ˆ](#å®ç°æ–¹æ¡ˆ)
6. [ä»£ç ç¤ºä¾‹](#ä»£ç ç¤ºä¾‹)
7. [æµ‹è¯•è®¡åˆ’](#æµ‹è¯•è®¡åˆ’)

---

## 1. ç°çŠ¶åˆ†æ

### å½“å‰å®ç°çŠ¶æ€

**âœ… å·²æ”¯æŒçš„åŠŸèƒ½ï¼š**

1. **Google Search å·¥å…·è‡ªåŠ¨æ³¨å…¥**
   - ä½ç½®ï¼š`src/openai_transfer.py:162`
   - å¯¹äºæœç´¢æ¨¡å‹ï¼ˆå¸¦ `-search` åç¼€ï¼‰ï¼Œè‡ªåŠ¨æ·»åŠ  `{"googleSearch": {}}`

2. **Gemini åŸç”Ÿæ ¼å¼å®Œå…¨é€ä¼ **
   - ä½ç½®ï¼š`src/google_chat_api.py:515-524`
   - æ”¯æŒ `tools` å­—æ®µå®Œå…¨é€ä¼ åˆ° Gemini API
   - æ”¯æŒ `toolConfig` å­—æ®µå®Œå…¨é€ä¼ 
   - æ”¯æŒ `cachedContent` å­—æ®µå®Œå…¨é€ä¼ 

3. **æ•°æ®æ¨¡å‹å·²å®šä¹‰**
   - ä½ç½®ï¼š`src/models.py:116-117`
   ```python
   tools: Optional[List[Dict[str, Any]]] = None
   toolConfig: Optional[Dict[str, Any]] = None
   ```

**âŒ ç¼ºå¤±çš„åŠŸèƒ½ï¼š**

1. **OpenAI â†’ Gemini å·¥å…·æ ¼å¼è½¬æ¢**
   - OpenAI çš„ `tools` ä½¿ç”¨ `type: "function"` + `function` å¯¹è±¡
   - Gemini çš„ `tools` ä½¿ç”¨ `functionDeclarations` æ•°ç»„
   - **å½“å‰ä¸æ”¯æŒè‡ªåŠ¨è½¬æ¢**

2. **å·¥å…·è°ƒç”¨å“åº”è½¬æ¢**
   - Gemini è¿”å› `functionCall` å¯¹è±¡
   - OpenAI è¿”å› `tool_calls` æ•°ç»„
   - **å½“å‰ä¸æ”¯æŒå“åº”æ ¼å¼è½¬æ¢**

3. **å·¥å…·æ‰§è¡Œç»“æœå¤„ç†**
   - OpenAI ä½¿ç”¨ `tool` role çš„æ¶ˆæ¯
   - Gemini ä½¿ç”¨ `functionResponse` å¯¹è±¡
   - **å½“å‰ä¸æ”¯æŒå¤šè½®å¯¹è¯çš„å·¥å…·ç»“æœå¤„ç†**

---

## 2. OpenAI å·¥å…·è°ƒç”¨æ ¼å¼

### è¯·æ±‚æ ¼å¼

```json
{
  "model": "gpt-4",
  "messages": [
    {
      "role": "user",
      "content": "What's the weather in Boston?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"],
              "description": "The temperature unit to use"
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "tool_choice": "auto"
}
```

### å“åº”æ ¼å¼

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1699896916,
  "model": "gpt-4",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": null,
        "tool_calls": [
          {
            "id": "call_abc123",
            "type": "function",
            "function": {
              "name": "get_current_weather",
              "arguments": "{\"location\":\"Boston, MA\"}"
            }
          }
        ]
      },
      "finish_reason": "tool_calls"
    }
  ]
}
```

### å·¥å…·ç»“æœè¿”å›æ ¼å¼

```json
{
  "model": "gpt-4",
  "messages": [
    {
      "role": "user",
      "content": "What's the weather in Boston?"
    },
    {
      "role": "assistant",
      "content": null,
      "tool_calls": [
        {
          "id": "call_abc123",
          "type": "function",
          "function": {
            "name": "get_current_weather",
            "arguments": "{\"location\":\"Boston, MA\"}"
          }
        }
      ]
    },
    {
      "role": "tool",
      "tool_call_id": "call_abc123",
      "name": "get_current_weather",
      "content": "{\"temperature\": 22, \"unit\": \"celsius\", \"description\": \"Sunny\"}"
    }
  ]
}
```

### tool_choice é€‰é¡¹

- `"auto"` (é»˜è®¤): æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
- `"none"`: å¼ºåˆ¶æ¨¡å‹ä¸è°ƒç”¨å·¥å…·
- `{"type": "function", "function": {"name": "my_function"}}`: å¼ºåˆ¶è°ƒç”¨ç‰¹å®šå·¥å…·
- `"required"`: å¼ºåˆ¶æ¨¡å‹å¿…é¡»è°ƒç”¨è‡³å°‘ä¸€ä¸ªå·¥å…·

---

## 3. Gemini å·¥å…·è°ƒç”¨æ ¼å¼

### è¯·æ±‚æ ¼å¼

```json
{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "What's the weather in Boston?"
        }
      ]
    }
  ],
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "get_current_weather",
          "description": "Get the current weather in a given location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA"
              },
              "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "The temperature unit to use"
              }
            },
            "required": ["location"]
          }
        }
      ]
    }
  ],
  "toolConfig": {
    "functionCallingConfig": {
      "mode": "AUTO"
    }
  }
}
```

### å“åº”æ ¼å¼

```json
{
  "candidates": [
    {
      "content": {
        "role": "model",
        "parts": [
          {
            "functionCall": {
              "name": "get_current_weather",
              "args": {
                "location": "Boston, MA"
              }
            }
          }
        ]
      },
      "finishReason": "STOP"
    }
  ]
}
```

### å·¥å…·ç»“æœè¿”å›æ ¼å¼

```json
{
  "contents": [
    {
      "role": "user",
      "parts": [{"text": "What's the weather in Boston?"}]
    },
    {
      "role": "model",
      "parts": [
        {
          "functionCall": {
            "name": "get_current_weather",
            "args": {"location": "Boston, MA"}
          }
        }
      ]
    },
    {
      "role": "user",
      "parts": [
        {
          "functionResponse": {
            "name": "get_current_weather",
            "response": {
              "temperature": 22,
              "unit": "celsius",
              "description": "Sunny"
            }
          }
        }
      ]
    }
  ]
}
```

### toolConfig æ¨¡å¼

```json
{
  "functionCallingConfig": {
    "mode": "AUTO|ANY|NONE",
    "allowedFunctionNames": ["function1", "function2"]
  }
}
```

- `AUTO` (é»˜è®¤): æ¨¡å‹è‡ªåŠ¨å†³å®š
- `ANY`: å¿…é¡»è°ƒç”¨æŸä¸ªå‡½æ•°
- `NONE`: ç¦ç”¨å‡½æ•°è°ƒç”¨

---

## 4. æ ¼å¼å¯¹æ¯”

| ç‰¹æ€§ | OpenAI | Gemini | è½¬æ¢å¤æ‚åº¦ |
|------|--------|--------|-----------|
| **å·¥å…·å®šä¹‰ä½ç½®** | `tools[].function` | `tools[].functionDeclarations[]` | ğŸŸ¡ ä¸­ç­‰ |
| **ç±»å‹å£°æ˜** | `type: "function"` | æ— éœ€ç±»å‹å­—æ®µ | ğŸŸ¢ ç®€å• |
| **å‚æ•°æ ¼å¼** | JSON Schema | JSON Schema (å­é›†) | ğŸŸ¢ ç®€å• |
| **å·¥å…·é€‰æ‹©** | `tool_choice` | `toolConfig.functionCallingConfig` | ğŸŸ¡ ä¸­ç­‰ |
| **å“åº”æ ¼å¼** | `tool_calls[]` æ•°ç»„ | `parts[].functionCall` | ğŸŸ¡ ä¸­ç­‰ |
| **å·¥å…· ID** | å¿…éœ€ (`id` å­—æ®µ) | ä¸éœ€è¦ | ğŸŸ¢ ç®€å• |
| **å‚æ•°ç¼–ç ** | JSON å­—ç¬¦ä¸² | JSON å¯¹è±¡ | ğŸŸ¢ ç®€å• |
| **å·¥å…·ç»“æœ** | `role: "tool"` æ¶ˆæ¯ | `functionResponse` å¯¹è±¡ | ğŸŸ¡ ä¸­ç­‰ |

### å…³é”®å·®å¼‚

1. **ç»“æ„åµŒå¥—**
   - OpenAI: `tools[i].function` åŒ…å«å‡½æ•°å®šä¹‰
   - Gemini: `tools[i].functionDeclarations[]` æ•°ç»„

2. **å‚æ•°ç¼–ç **
   - OpenAI: `arguments` æ˜¯ JSON å­—ç¬¦ä¸²
   - Gemini: `args` æ˜¯ JSON å¯¹è±¡

3. **å·¥å…· ID**
   - OpenAI: éœ€è¦ç”Ÿæˆå”¯ä¸€ ID (`call_xxx`)
   - Gemini: ä¸éœ€è¦ ID

4. **å“åº”ä½ç½®**
   - OpenAI: `message.tool_calls[]` ç‹¬ç«‹æ•°ç»„
   - Gemini: `parts[]` æ•°ç»„ä¸­çš„ä¸€ä¸ª part

---

## 5. å®ç°æ–¹æ¡ˆ

### æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OpenAI Request                            â”‚
â”‚  tools: [{type: "function", function: {...}}]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            openai_request_to_gemini_payload()               â”‚
â”‚  - è½¬æ¢ tools æ ¼å¼                                           â”‚
â”‚  - è½¬æ¢ tool_choice â†’ toolConfig                            â”‚
â”‚  - å¤„ç† tool role æ¶ˆæ¯ â†’ functionResponse                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Gemini Request                           â”‚
â”‚  tools: [{functionDeclarations: [{...}]}]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                  [Google API]
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Gemini Response                           â”‚
â”‚  parts: [{functionCall: {...}}]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            gemini_response_to_openai()                      â”‚
â”‚  - è½¬æ¢ functionCall â†’ tool_calls                           â”‚
â”‚  - ç”Ÿæˆ tool_call_id                                        â”‚
â”‚  - è®¾ç½® finish_reason                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OpenAI Response                           â”‚
â”‚  tool_calls: [{id: "...", function: {...}}]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®ç°æ­¥éª¤

#### Step 1: è¯·æ±‚è½¬æ¢ - å·¥å…·å®šä¹‰

åœ¨ `src/openai_transfer.py` çš„ `openai_request_to_gemini_payload()` å‡½æ•°ä¸­æ·»åŠ ï¼š

```python
# è½¬æ¢ OpenAI tools åˆ° Gemini functionDeclarations
if hasattr(openai_request, 'tools') and openai_request.tools:
    gemini_tools = convert_openai_tools_to_gemini(openai_request.tools)
    if gemini_tools:
        request_data["tools"] = gemini_tools

# è½¬æ¢ tool_choice åˆ° toolConfig
if hasattr(openai_request, 'tool_choice') and openai_request.tool_choice:
    request_data["toolConfig"] = convert_tool_choice_to_tool_config(
        openai_request.tool_choice
    )
```

#### Step 2: è¯·æ±‚è½¬æ¢ - å·¥å…·æ¶ˆæ¯

å¤„ç† `role: "tool"` çš„æ¶ˆæ¯ï¼š

```python
for message in openai_request.messages:
    role = message.role

    if role == "tool":
        # è½¬æ¢å·¥å…·ç»“æœæ¶ˆæ¯
        function_response = {
            "functionResponse": {
                "name": message.name,
                "response": json.loads(message.content)
            }
        }
        contents.append({
            "role": "user",  # Gemini ä¸­å·¥å…·å“åº”ä½œä¸º user æ¶ˆæ¯
            "parts": [function_response]
        })
        continue
```

#### Step 3: å“åº”è½¬æ¢ - å·¥å…·è°ƒç”¨

åœ¨ `gemini_response_to_openai()` å’Œ `gemini_stream_chunk_to_openai()` ä¸­æ·»åŠ ï¼š

```python
# æ£€æŸ¥æ˜¯å¦åŒ…å«å‡½æ•°è°ƒç”¨
tool_calls = []
regular_content = ""

for part in parts:
    if "functionCall" in part:
        # è½¬æ¢ä¸º OpenAI æ ¼å¼
        tool_call = {
            "id": f"call_{uuid.uuid4().hex[:24]}",
            "type": "function",
            "function": {
                "name": part["functionCall"]["name"],
                "arguments": json.dumps(part["functionCall"]["args"])
            }
        }
        tool_calls.append(tool_call)
    elif "text" in part:
        regular_content += part["text"]

# æ„å»ºæ¶ˆæ¯
message = {"role": role}
if tool_calls:
    message["tool_calls"] = tool_calls
    message["content"] = regular_content if regular_content else None
    finish_reason = "tool_calls"
else:
    message["content"] = regular_content
```

#### Step 4: æ•°æ®æ¨¡å‹æ›´æ–°

åœ¨ `src/models.py` ä¸­æ·»åŠ ï¼š

```python
class OpenAITool(BaseModel):
    type: str = "function"
    function: Dict[str, Any]

class OpenAIToolCall(BaseModel):
    id: str
    type: str = "function"
    function: Dict[str, Any]

class OpenAIChatMessage(BaseModel):
    role: str
    content: Union[str, List[Dict[str, Any]], None] = None
    tool_calls: Optional[List[OpenAIToolCall]] = None
    tool_call_id: Optional[str] = None  # for role="tool"
    name: Optional[str] = None  # function name for role="tool"
    # ... existing fields

class ChatCompletionRequest(BaseModel):
    # ... existing fields
    tools: Optional[List[OpenAITool]] = None
    tool_choice: Optional[Union[str, Dict[str, Any]]] = None
```

---

## 6. ä»£ç ç¤ºä¾‹

### å®Œæ•´çš„è½¬æ¢å‡½æ•°å®ç°

#### å·¥å…·å®šä¹‰è½¬æ¢

```python
def convert_openai_tools_to_gemini(openai_tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å°† OpenAI tools æ ¼å¼è½¬æ¢ä¸º Gemini functionDeclarations æ ¼å¼

    Args:
        openai_tools: OpenAI æ ¼å¼çš„å·¥å…·åˆ—è¡¨

    Returns:
        Gemini æ ¼å¼çš„å·¥å…·åˆ—è¡¨
    """
    if not openai_tools:
        return []

    function_declarations = []

    for tool in openai_tools:
        if tool.get("type") != "function":
            log.warning(f"Skipping non-function tool type: {tool.get('type')}")
            continue

        function = tool.get("function")
        if not function:
            log.warning("Tool missing 'function' field")
            continue

        # æ„å»º Gemini function declaration
        declaration = {
            "name": function.get("name"),
            "description": function.get("description", ""),
            "parameters": function.get("parameters", {})
        }

        function_declarations.append(declaration)

    if not function_declarations:
        return []

    # Gemini æ ¼å¼ï¼šå·¥å…·æ•°ç»„ä¸­åŒ…å« functionDeclarations
    return [{"functionDeclarations": function_declarations}]


def convert_tool_choice_to_tool_config(tool_choice: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
    """
    å°† OpenAI tool_choice è½¬æ¢ä¸º Gemini toolConfig

    Args:
        tool_choice: OpenAI æ ¼å¼çš„ tool_choice

    Returns:
        Gemini æ ¼å¼çš„ toolConfig
    """
    if isinstance(tool_choice, str):
        if tool_choice == "auto":
            return {
                "functionCallingConfig": {
                    "mode": "AUTO"
                }
            }
        elif tool_choice == "none":
            return {
                "functionCallingConfig": {
                    "mode": "NONE"
                }
            }
        elif tool_choice == "required":
            return {
                "functionCallingConfig": {
                    "mode": "ANY"
                }
            }
    elif isinstance(tool_choice, dict):
        # {"type": "function", "function": {"name": "my_function"}}
        if tool_choice.get("type") == "function":
            function_name = tool_choice.get("function", {}).get("name")
            if function_name:
                return {
                    "functionCallingConfig": {
                        "mode": "ANY",
                        "allowedFunctionNames": [function_name]
                    }
                }

    # é»˜è®¤è¿”å› AUTO æ¨¡å¼
    return {
        "functionCallingConfig": {
            "mode": "AUTO"
        }
    }
```

#### æ¶ˆæ¯è½¬æ¢ï¼ˆåŒ…å«å·¥å…·ç»“æœï¼‰

```python
def convert_tool_message_to_function_response(message: OpenAIChatMessage) -> Dict[str, Any]:
    """
    å°† OpenAI çš„ tool role æ¶ˆæ¯è½¬æ¢ä¸º Gemini functionResponse

    Args:
        message: OpenAI æ ¼å¼çš„å·¥å…·æ¶ˆæ¯

    Returns:
        Gemini æ ¼å¼çš„ functionResponse part
    """
    try:
        # å°è¯•å°† content è§£æä¸º JSON
        response_data = json.loads(message.content) if isinstance(message.content, str) else message.content
    except json.JSONDecodeError:
        # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼ŒåŒ…è£…ä¸ºå¯¹è±¡
        response_data = {"result": message.content}

    return {
        "functionResponse": {
            "name": message.name,
            "response": response_data
        }
    }
```

#### å“åº”è½¬æ¢ï¼ˆæå–å·¥å…·è°ƒç”¨ï¼‰

```python
def extract_tool_calls_from_parts(parts: List[Dict[str, Any]]) -> tuple[List[Dict[str, Any]], str]:
    """
    ä» Gemini response parts ä¸­æå–å·¥å…·è°ƒç”¨å’Œæ–‡æœ¬å†…å®¹

    Args:
        parts: Gemini response çš„ parts æ•°ç»„

    Returns:
        (tool_calls, text_content) å…ƒç»„
    """
    tool_calls = []
    text_content = ""

    for part in parts:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å‡½æ•°è°ƒç”¨
        if "functionCall" in part:
            function_call = part["functionCall"]
            tool_call = {
                "id": f"call_{uuid.uuid4().hex[:24]}",
                "type": "function",
                "function": {
                    "name": function_call.get("name"),
                    "arguments": json.dumps(function_call.get("args", {}))
                }
            }
            tool_calls.append(tool_call)

        # æå–æ–‡æœ¬å†…å®¹ï¼ˆæ’é™¤ thinking tokensï¼‰
        elif "text" in part and not part.get("thought", False):
            text_content += part["text"]

    return tool_calls, text_content
```

#### å®Œæ•´çš„å“åº”è½¬æ¢

```python
def gemini_response_to_openai_with_tools(
    gemini_response: Dict[str, Any],
    model: str
) -> Dict[str, Any]:
    """
    å°†åŒ…å«å·¥å…·è°ƒç”¨çš„ Gemini å“åº”è½¬æ¢ä¸º OpenAI æ ¼å¼

    Args:
        gemini_response: Gemini API å“åº”
        model: æ¨¡å‹åç§°

    Returns:
        OpenAI æ ¼å¼çš„å“åº”
    """
    choices = []

    for candidate in gemini_response.get("candidates", []):
        role = candidate.get("content", {}).get("role", "assistant")
        if role == "model":
            role = "assistant"

        parts = candidate.get("content", {}).get("parts", [])

        # æå–å·¥å…·è°ƒç”¨å’Œæ–‡æœ¬å†…å®¹
        tool_calls, text_content = extract_tool_calls_from_parts(parts)

        # æå– reasoning contentï¼ˆthinking tokensï¼‰
        reasoning_content = ""
        for part in parts:
            if part.get("thought", False) and "text" in part:
                reasoning_content += part["text"]

        # æ„å»ºæ¶ˆæ¯
        message = {"role": role}

        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨
        if tool_calls:
            message["tool_calls"] = tool_calls
            # content å¯ä»¥æ˜¯ None æˆ–åŒ…å«æ–‡æœ¬
            message["content"] = text_content if text_content else None
            finish_reason = "tool_calls"
        else:
            message["content"] = text_content
            finish_reason = _map_finish_reason(candidate.get("finishReason"))

        # æ·»åŠ  reasoning contentï¼ˆå¦‚æœæœ‰ï¼‰
        if reasoning_content:
            message["reasoning_content"] = reasoning_content

        choices.append({
            "index": candidate.get("index", 0),
            "message": message,
            "finish_reason": finish_reason
        })

    # è½¬æ¢ usage metadata
    usage = _convert_usage_metadata(gemini_response.get("usageMetadata"))

    response_data = {
        "id": str(uuid.uuid4()),
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": choices
    }

    if usage:
        response_data["usage"] = usage

    return response_data
```

#### æµå¼å“åº”è½¬æ¢

```python
def gemini_stream_chunk_to_openai_with_tools(
    gemini_chunk: Dict[str, Any],
    model: str,
    response_id: str
) -> Dict[str, Any]:
    """
    å°†åŒ…å«å·¥å…·è°ƒç”¨çš„ Gemini æµå¼å“åº”è½¬æ¢ä¸º OpenAI æ ¼å¼

    Args:
        gemini_chunk: Gemini æµå¼å“åº”å—
        model: æ¨¡å‹åç§°
        response_id: å“åº” ID

    Returns:
        OpenAI æµå¼æ ¼å¼
    """
    choices = []

    for candidate in gemini_chunk.get("candidates", []):
        role = candidate.get("content", {}).get("role", "assistant")
        if role == "model":
            role = "assistant"

        parts = candidate.get("content", {}).get("parts", [])

        # æå–å·¥å…·è°ƒç”¨å’Œæ–‡æœ¬
        tool_calls, text_content = extract_tool_calls_from_parts(parts)

        # æå– reasoning content
        reasoning_content = ""
        for part in parts:
            if part.get("thought", False) and "text" in part:
                reasoning_content += part["text"]

        # æ„å»º delta
        delta = {}

        if tool_calls:
            # æµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨
            delta["tool_calls"] = tool_calls
            if text_content:
                delta["content"] = text_content
        elif text_content:
            delta["content"] = text_content

        if reasoning_content:
            delta["reasoning_content"] = reasoning_content

        finish_reason = _map_finish_reason(candidate.get("finishReason"))
        if finish_reason == "STOP" and tool_calls:
            finish_reason = "tool_calls"

        choices.append({
            "index": candidate.get("index", 0),
            "delta": delta,
            "finish_reason": finish_reason
        })

    # è½¬æ¢ usage
    usage = _convert_usage_metadata(gemini_chunk.get("usageMetadata"))

    response_data = {
        "id": response_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": choices
    }

    if usage:
        has_finish_reason = any(choice.get("finish_reason") for choice in choices)
        if has_finish_reason:
            response_data["usage"] = usage

    return response_data
```

---

## 7. æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•

#### æµ‹è¯• 1: å·¥å…·å®šä¹‰è½¬æ¢

```python
def test_convert_openai_tools_to_gemini():
    openai_tools = [
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get current weather",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "City name"
                        }
                    },
                    "required": ["location"]
                }
            }
        }
    ]

    result = convert_openai_tools_to_gemini(openai_tools)

    assert len(result) == 1
    assert "functionDeclarations" in result[0]
    assert len(result[0]["functionDeclarations"]) == 1
    assert result[0]["functionDeclarations"][0]["name"] == "get_weather"
```

#### æµ‹è¯• 2: tool_choice è½¬æ¢

```python
def test_convert_tool_choice():
    # æµ‹è¯• "auto"
    result = convert_tool_choice_to_tool_config("auto")
    assert result["functionCallingConfig"]["mode"] == "AUTO"

    # æµ‹è¯• "required"
    result = convert_tool_choice_to_tool_config("required")
    assert result["functionCallingConfig"]["mode"] == "ANY"

    # æµ‹è¯•æŒ‡å®šå‡½æ•°
    result = convert_tool_choice_to_tool_config({
        "type": "function",
        "function": {"name": "my_func"}
    })
    assert result["functionCallingConfig"]["mode"] == "ANY"
    assert "my_func" in result["functionCallingConfig"]["allowedFunctionNames"]
```

#### æµ‹è¯• 3: å·¥å…·è°ƒç”¨å“åº”æå–

```python
def test_extract_tool_calls():
    parts = [
        {
            "functionCall": {
                "name": "get_weather",
                "args": {"location": "Boston"}
            }
        },
        {
            "text": "Let me check the weather for you."
        }
    ]

    tool_calls, text = extract_tool_calls_from_parts(parts)

    assert len(tool_calls) == 1
    assert tool_calls[0]["function"]["name"] == "get_weather"
    assert "Boston" in tool_calls[0]["function"]["arguments"]
    assert "Let me check" in text
```

### é›†æˆæµ‹è¯•

#### æµ‹è¯• 4: å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹

```python
async def test_full_tool_calling_flow():
    """æµ‹è¯•ä» OpenAI è¯·æ±‚åˆ°å·¥å…·è°ƒç”¨å“åº”çš„å®Œæ•´æµç¨‹"""

    # 1. å‡†å¤‡ OpenAI è¯·æ±‚
    request = {
        "model": "gemini-2.5-flash-preview",
        "messages": [
            {
                "role": "user",
                "content": "What's the weather in Tokyo?"
            }
        ],
        "tools": [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather information",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {"type": "string"}
                        },
                        "required": ["location"]
                    }
                }
            }
        ],
        "tool_choice": "auto"
    }

    # 2. è½¬æ¢ä¸º Gemini æ ¼å¼
    openai_req = ChatCompletionRequest(**request)
    gemini_payload = await openai_request_to_gemini_payload(openai_req)

    # 3. éªŒè¯è½¬æ¢ç»“æœ
    assert "tools" in gemini_payload["request"]
    assert "functionDeclarations" in gemini_payload["request"]["tools"][0]
    assert "toolConfig" in gemini_payload["request"]

    # 4. æ¨¡æ‹Ÿ Gemini å“åº”
    gemini_response = {
        "candidates": [
            {
                "content": {
                    "role": "model",
                    "parts": [
                        {
                            "functionCall": {
                                "name": "get_weather",
                                "args": {"location": "Tokyo"}
                            }
                        }
                    ]
                },
                "finishReason": "STOP"
            }
        ]
    }

    # 5. è½¬æ¢å› OpenAI æ ¼å¼
    openai_response = gemini_response_to_openai_with_tools(
        gemini_response,
        request["model"]
    )

    # 6. éªŒè¯å“åº”
    assert len(openai_response["choices"]) == 1
    choice = openai_response["choices"][0]
    assert "tool_calls" in choice["message"]
    assert len(choice["message"]["tool_calls"]) == 1
    assert choice["message"]["tool_calls"][0]["function"]["name"] == "get_weather"
    assert choice["finish_reason"] == "tool_calls"
```

#### æµ‹è¯• 5: å·¥å…·ç»“æœçš„å¤šè½®å¯¹è¯

```python
async def test_multi_turn_with_tool_result():
    """æµ‹è¯•åŒ…å«å·¥å…·æ‰§è¡Œç»“æœçš„å¤šè½®å¯¹è¯"""

    request = {
        "model": "gemini-2.5-flash-preview",
        "messages": [
            {
                "role": "user",
                "content": "What's the weather in Tokyo?"
            },
            {
                "role": "assistant",
                "content": None,
                "tool_calls": [
                    {
                        "id": "call_abc123",
                        "type": "function",
                        "function": {
                            "name": "get_weather",
                            "arguments": '{"location": "Tokyo"}'
                        }
                    }
                ]
            },
            {
                "role": "tool",
                "tool_call_id": "call_abc123",
                "name": "get_weather",
                "content": '{"temperature": 18, "condition": "Cloudy"}'
            }
        ],
        "tools": [...]
    }

    openai_req = ChatCompletionRequest(**request)
    gemini_payload = await openai_request_to_gemini_payload(openai_req)

    # éªŒè¯å·¥å…·ç»“æœè¢«æ­£ç¡®è½¬æ¢
    contents = gemini_payload["request"]["contents"]

    # åº”è¯¥æœ‰ 3 æ¡æ¶ˆæ¯ï¼šuser, model (with functionCall), user (with functionResponse)
    assert len(contents) == 3

    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯åŒ…å« functionResponse
    last_message = contents[-1]
    assert last_message["role"] == "user"
    assert "functionResponse" in last_message["parts"][0]
    assert last_message["parts"][0]["functionResponse"]["name"] == "get_weather"
```

### ç«¯åˆ°ç«¯æµ‹è¯•

#### æµ‹è¯• 6: å®é™… API è°ƒç”¨æµ‹è¯•

```bash
# æµ‹è¯•å·¥å…·è°ƒç”¨
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash-preview",
    "messages": [
      {
        "role": "user",
        "content": "What is 15 * 7?"
      }
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate",
          "description": "Perform mathematical calculation",
          "parameters": {
            "type": "object",
            "properties": {
              "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
              }
            },
            "required": ["expression"]
          }
        }
      }
    ]
  }'
```

#### æµ‹è¯• 7: æµå¼å·¥å…·è°ƒç”¨

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash-preview",
    "messages": [...],
    "tools": [...],
    "stream": true
  }'
```

### è¾¹ç•Œæƒ…å†µæµ‹è¯•

#### æµ‹è¯• 8: ç©ºå·¥å…·åˆ—è¡¨

```python
def test_empty_tools():
    result = convert_openai_tools_to_gemini([])
    assert result == []
```

#### æµ‹è¯• 9: æ— æ•ˆçš„å·¥å…·ç±»å‹

```python
def test_invalid_tool_type():
    tools = [{"type": "invalid_type"}]
    result = convert_openai_tools_to_gemini(tools)
    assert result == []
```

#### æµ‹è¯• 10: å¤šä¸ªå·¥å…·å®šä¹‰

```python
def test_multiple_tools():
    tools = [
        {"type": "function", "function": {"name": "tool1", ...}},
        {"type": "function", "function": {"name": "tool2", ...}}
    ]
    result = convert_openai_tools_to_gemini(tools)
    assert len(result) == 1
    assert len(result[0]["functionDeclarations"]) == 2
```

---

## 8. å®ç°æ£€æŸ¥æ¸…å•

### ä»£ç ä¿®æ”¹

- [ ] æ›´æ–° `src/models.py` - æ·»åŠ å·¥å…·ç›¸å…³çš„æ•°æ®æ¨¡å‹
- [ ] æ›´æ–° `src/openai_transfer.py` - æ·»åŠ å·¥å…·è½¬æ¢å‡½æ•°
- [ ] æ›´æ–° `openai_request_to_gemini_payload()` - é›†æˆå·¥å…·è½¬æ¢
- [ ] æ›´æ–° `gemini_response_to_openai()` - æ·»åŠ å·¥å…·è°ƒç”¨æå–
- [ ] æ›´æ–° `gemini_stream_chunk_to_openai()` - æ·»åŠ æµå¼å·¥å…·è°ƒç”¨æ”¯æŒ
- [ ] æ·»åŠ  `src/tool_converter.py` - ç‹¬ç«‹çš„å·¥å…·è½¬æ¢æ¨¡å—ï¼ˆå¯é€‰ï¼‰

### æµ‹è¯•

- [ ] ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] ç¼–å†™é›†æˆæµ‹è¯•
- [ ] ç¼–å†™ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] æµ‹è¯•è¾¹ç•Œæƒ…å†µ
- [ ] æµ‹è¯•é”™è¯¯å¤„ç†

### æ–‡æ¡£

- [ ] æ›´æ–° README.md - æ·»åŠ å·¥å…·è°ƒç”¨ä½¿ç”¨è¯´æ˜
- [ ] æ·»åŠ ç¤ºä¾‹ä»£ç 
- [ ] æ›´æ–° API æ–‡æ¡£
- [ ] æ·»åŠ æ•…éšœæ’é™¤æŒ‡å—

### éƒ¨ç½²

- [ ] æœ¬åœ°æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] å‘åå…¼å®¹æ€§æ£€æŸ¥
- [ ] éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

---

## 9. æ³¨æ„äº‹é¡¹å’Œé™åˆ¶

### Gemini API é™åˆ¶

1. **ä¸æ”¯æŒçš„ JSON Schema ç‰¹æ€§**
   - `default` å­—æ®µ
   - `optional` å­—æ®µ
   - `maximum`/`minimum` å­—æ®µ
   - `oneOf`/`anyOf`/`allOf`

2. **å·¥å…·è°ƒç”¨é™åˆ¶**
   - æœ€å¤šå¯ä»¥å®šä¹‰å¤šå°‘ä¸ªå‡½æ•°ï¼ˆéœ€è¦æŸ¥é˜…å®˜æ–¹æ–‡æ¡£ï¼‰
   - å‚æ•°å¤§å°é™åˆ¶

### OpenAI å…¼å®¹æ€§

1. **tool_call_id ç”Ÿæˆ**
   - éœ€è¦ç”Ÿæˆå”¯ä¸€çš„ ID
   - æ ¼å¼ï¼š`call_` + 24 ä½åå…­è¿›åˆ¶

2. **parallel_tool_calls**
   - OpenAI æ”¯æŒ `parallel_tool_calls` å‚æ•°
   - Gemini å¯èƒ½æœ‰ä¸åŒçš„è¡Œä¸º

### é”™è¯¯å¤„ç†

1. **å·¥å…·å®šä¹‰éªŒè¯**
   - éªŒè¯å¿…éœ€å­—æ®µ
   - å¤„ç†æ— æ•ˆçš„ schema

2. **å·¥å…·è°ƒç”¨å¤±è´¥**
   - å¤„ç† Gemini è¿”å›çš„é”™è¯¯
   - è½¬æ¢ä¸º OpenAI æ ¼å¼çš„é”™è¯¯

---

## 10. æ€»ç»“

### å½“å‰çŠ¶æ€

âœ… **å·²å®Œæˆï¼š**
- Google Search å·¥å…·è‡ªåŠ¨æ³¨å…¥
- Gemini åŸç”Ÿæ ¼å¼é€ä¼ 
- åŸºç¡€æ•°æ®æ¨¡å‹å®šä¹‰

âŒ **å¾…å®ç°ï¼š**
- OpenAI â†’ Gemini å·¥å…·æ ¼å¼è½¬æ¢
- Gemini â†’ OpenAI å·¥å…·è°ƒç”¨å“åº”è½¬æ¢
- å¤šè½®å¯¹è¯ä¸­çš„å·¥å…·ç»“æœå¤„ç†
- å®Œæ•´çš„æµ‹è¯•è¦†ç›–

### å®ç°ä¼˜å…ˆçº§

1. **é«˜ä¼˜å…ˆçº§ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰**
   - å·¥å…·å®šä¹‰æ ¼å¼è½¬æ¢
   - å·¥å…·è°ƒç”¨å“åº”è½¬æ¢
   - åŸºæœ¬çš„å•è½®å·¥å…·è°ƒç”¨

2. **ä¸­ä¼˜å…ˆçº§ï¼ˆå®Œæ•´ä½“éªŒï¼‰**
   - å¤šè½®å¯¹è¯æ”¯æŒ
   - å·¥å…·ç»“æœå¤„ç†
   - tool_choice è½¬æ¢

3. **ä½ä¼˜å…ˆçº§ï¼ˆä¼˜åŒ–ï¼‰**
   - å¹¶è¡Œå·¥å…·è°ƒç”¨
   - é«˜çº§ toolConfig é€‰é¡¹
   - æ€§èƒ½ä¼˜åŒ–

### é¢„æœŸæ•ˆæœ

å®ç°å®Œæˆåï¼Œç”¨æˆ·å¯ä»¥ï¼š

1. ä½¿ç”¨ OpenAI çš„å·¥å…·è°ƒç”¨æ ¼å¼ä¸ Gemini æ¨¡å‹äº¤äº’
2. æ— éœ€ä¿®æ”¹ç°æœ‰çš„ OpenAI å®¢æˆ·ç«¯ä»£ç 
3. äº«å— Gemini çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼ˆå¦‚ Google Searchï¼‰
4. åœ¨æµå¼å’Œéæµå¼æ¨¡å¼ä¸‹éƒ½èƒ½æ­£å¸¸å·¥ä½œ

---

## å‚è€ƒèµ„æ–™

- [Google Gemini API Function Calling Documentation](https://ai.google.dev/gemini-api/docs/function-calling)
- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)
- [JSON Schema Specification](https://json-schema.org/)
- gcli2api æºä»£ç ï¼š`src/openai_transfer.py`, `src/models.py`, `src/google_chat_api.py`
